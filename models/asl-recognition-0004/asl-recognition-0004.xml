<?xml version="1.0" ?>
<net name="asl-recognition-0004" version="10">
	<layers>
		<layer id="0" name="input" type="Parameter" version="opset1">
			<data element_type="f32" shape="1,3,16,224,224"/>
			<output>
				<port id="0" names="input" precision="FP32">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
			</output>
		</layer>
		<layer id="1" name="data_mul_14168_const" type="Const" version="opset1">
			<data element_type="f32" offset="0" shape="1,3,1,1,1" size="12"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="2" name="input/scale/Fused_Mul_" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
			</output>
		</layer>
		<layer id="3" name="data_add_14170_const" type="Const" version="opset1">
			<data element_type="f32" offset="12" shape="1,3,1,1,1" size="12"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="4" name="input/mean/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
			</output>
		</layer>
		<layer id="5" name="368/mean/Fused_Mul_1603716039_const" type="Const" version="opset1">
			<data element_type="f32" offset="24" shape="16,3,1,3,3" size="1728"/>
			<output>
				<port id="0" names="backbone.features.0.0.weight" precision="FP32">
					<dim>16</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="6" name="367" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,2,2"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>3</dim>
					<dim>16</dim>
					<dim>224</dim>
					<dim>224</dim>
				</port>
				<port id="1">
					<dim>16</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="7" name="data_add_141731417814853/EltwiseUnsqueeze15245_const" type="Const" version="opset1">
			<data element_type="f32" offset="1752" shape="1,16,1,1,1" size="64"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="8" name="368/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="368" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="9" name="369/EltwiseUnsqueeze14973_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="369" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="10" name="370" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="370" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="11" name="371" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</input>
			<output>
				<port id="1" names="371" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="12" name="372/EltwiseUnsqueeze15113_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="372" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="13" name="373" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="373" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="14" name="374" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</input>
			<output>
				<port id="2" names="374" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="15" name="2610326106_const" type="Const" version="opset1">
			<data element_type="f32" offset="1824" shape="16,1,1,1,3,3" size="576"/>
			<output>
				<port id="0" names="backbone.features.1.conv.0.weight" precision="FP32">
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="16" name="375" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="17" name="data_add_141811418614855/EltwiseUnsqueeze15253_const" type="Const" version="opset1">
			<data element_type="f32" offset="2400" shape="1,16,1,1,1" size="64"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="18" name="376/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="376" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="19" name="377" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</input>
			<output>
				<port id="1" names="377" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="20" name="379/mean/Fused_Mul_1604516047_const" type="Const" version="opset1">
			<data element_type="f32" offset="2464" shape="16,16,5,1,1" size="5120"/>
			<output>
				<port id="0" names="backbone.features.1.conv.4.weight" precision="FP32">
					<dim>16</dim>
					<dim>16</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="21" name="378" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="2,0,0" pads_end="2,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>16</dim>
					<dim>16</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="22" name="data_add_141891419414857/EltwiseUnsqueeze15261_const" type="Const" version="opset1">
			<data element_type="f32" offset="7584" shape="1,16,1,1,1" size="64"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="23" name="379/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="379" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="24" name="380" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</input>
			<output>
				<port id="2" names="380" precision="FP32">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="25" name="382/mean/Fused_Mul_1604916051_const" type="Const" version="opset1">
			<data element_type="f32" offset="7648" shape="64,16,1,1,1" size="4096"/>
			<output>
				<port id="0" names="backbone.features.2.conv.0.weight" precision="FP32">
					<dim>64</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="26" name="381" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>16</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>64</dim>
					<dim>16</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="27" name="data_add_141971420214859/EltwiseUnsqueeze15269_const" type="Const" version="opset1">
			<data element_type="f32" offset="11744" shape="1,64,1,1,1" size="256"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="28" name="382/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="382" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="29" name="383" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</input>
			<output>
				<port id="1" names="383" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
			</output>
		</layer>
		<layer id="30" name="2607926082_const" type="Const" version="opset1">
			<data element_type="f32" offset="12000" shape="64,1,1,1,3,3" size="2304"/>
			<output>
				<port id="0" names="backbone.features.2.conv.3.weight" precision="FP32">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="31" name="384" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,2,2"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>112</dim>
					<dim>112</dim>
				</port>
				<port id="1">
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="32" name="data_add_142051421014861/EltwiseUnsqueeze15277_const" type="Const" version="opset1">
			<data element_type="f32" offset="14304" shape="1,64,1,1,1" size="256"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="33" name="385/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>64</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="385" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="34" name="386" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="1" names="386" precision="FP32">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="35" name="388/mean/Fused_Mul_1605716059_const" type="Const" version="opset1">
			<data element_type="f32" offset="14560" shape="24,64,3,1,1" size="18432"/>
			<output>
				<port id="0" names="backbone.features.2.conv.7.weight" precision="FP32">
					<dim>24</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="36" name="387" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>64</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>24</dim>
					<dim>64</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="37" name="data_add_142131421814863/EltwiseUnsqueeze15285_const" type="Const" version="opset1">
			<data element_type="f32" offset="32992" shape="1,24,1,1,1" size="96"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="38" name="388/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="389,388" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="39" name="390" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="2,1,1" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="2,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>16</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="1" names="390" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="40" name="392/mean/Fused_Mul_1606116063_const" type="Const" version="opset1">
			<data element_type="f32" offset="33088" shape="72,24,1,1,1" size="6912"/>
			<output>
				<port id="0" names="backbone.features.3.conv.0.weight" precision="FP32">
					<dim>72</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="41" name="391" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>72</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="42" name="data_add_142211422614865/EltwiseUnsqueeze15293_const" type="Const" version="opset1">
			<data element_type="f32" offset="40000" shape="1,72,1,1,1" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="43" name="392/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="392" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="44" name="393" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="1" names="393" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="45" name="2609526098_const" type="Const" version="opset1">
			<data element_type="f32" offset="40288" shape="72,1,1,1,3,3" size="2592"/>
			<output>
				<port id="0" names="backbone.features.3.conv.3.weight" precision="FP32">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="46" name="394" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="47" name="data_add_142291423414867/EltwiseUnsqueeze15301_const" type="Const" version="opset1">
			<data element_type="f32" offset="42880" shape="1,72,1,1,1" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="48" name="395/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="395" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="49" name="396" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="1" names="396" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="50" name="398/mean/Fused_Mul_1606916071_const" type="Const" version="opset1">
			<data element_type="f32" offset="43168" shape="24,72,3,1,1" size="20736"/>
			<output>
				<port id="0" names="backbone.features.3.conv.7.weight" precision="FP32">
					<dim>24</dim>
					<dim>72</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="51" name="397" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>24</dim>
					<dim>72</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="52" name="data_add_142371424214869/EltwiseUnsqueeze15309_const" type="Const" version="opset1">
			<data element_type="f32" offset="63904" shape="1,24,1,1,1" size="96"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="53" name="398/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="398" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="54" name="399" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="2" names="399" precision="FP32">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="55" name="401/mean/Fused_Mul_1607316075_const" type="Const" version="opset1">
			<data element_type="f32" offset="64000" shape="72,24,1,1,1" size="6912"/>
			<output>
				<port id="0" names="backbone.features.4.conv.0.weight" precision="FP32">
					<dim>72</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="56" name="400" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>24</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>72</dim>
					<dim>24</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="57" name="data_add_142451425014871/EltwiseUnsqueeze15317_const" type="Const" version="opset1">
			<data element_type="f32" offset="70912" shape="1,72,1,1,1" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="58" name="401/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="401" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="59" name="402" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</input>
			<output>
				<port id="1" names="402" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
			</output>
		</layer>
		<layer id="60" name="2610726110_const" type="Const" version="opset1">
			<data element_type="f32" offset="71200" shape="72,1,1,1,5,5" size="7200"/>
			<output>
				<port id="0" names="backbone.features.4.conv.3.weight" precision="FP32">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="61" name="403" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,2,2"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>56</dim>
					<dim>56</dim>
				</port>
				<port id="1">
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="62" name="data_add_142531425814873/EltwiseUnsqueeze15325_const" type="Const" version="opset1">
			<data element_type="f32" offset="78400" shape="1,72,1,1,1" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="63" name="404/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="405,404" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="64" name="406" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,28,28" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="406" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="65" name="backbone.features.4.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="78688" shape="18,72,1,1,1" size="5184"/>
			<output>
				<port id="0" names="backbone.features.4.conv.5.fc.0.weight" precision="FP32">
					<dim>18</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="66" name="407/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>18</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="67" name="407/Dims835514811/EltwiseUnsqueeze15085_const" type="Const" version="opset1">
			<data element_type="f32" offset="83872" shape="1,18,1,1,1" size="72"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>18</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="68" name="407" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>18</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="407" precision="FP32">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="69" name="408" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="408" precision="FP32">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="70" name="backbone.features.4.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="83944" shape="72,18,1,1,1" size="5184"/>
			<output>
				<port id="0" names="backbone.features.4.conv.5.fc.2.weight" precision="FP32">
					<dim>72</dim>
					<dim>18</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="71" name="409/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>18</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>72</dim>
					<dim>18</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="72" name="data_add_142601426214874/EltwiseUnsqueeze15329_const" type="Const" version="opset1">
			<data element_type="f32" offset="89128" shape="1,72,1,1,1" size="288"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="73" name="409/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="411" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="74" name="412" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="412" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="75" name="413/EltwiseUnsqueeze15117_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="413" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="76" name="414" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="414" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="77" name="415" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="415" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="78" name="416" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="416" precision="FP32">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="79" name="418/mean/Fused_Mul_1608116083_const" type="Const" version="opset1">
			<data element_type="f32" offset="89416" shape="40,72,3,1,1" size="34560"/>
			<output>
				<port id="0" names="backbone.features.4.conv.7.weight" precision="FP32">
					<dim>40</dim>
					<dim>72</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="80" name="417" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>72</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>40</dim>
					<dim>72</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="81" name="data_add_142651427014876/EltwiseUnsqueeze15337_const" type="Const" version="opset1">
			<data element_type="f32" offset="123976" shape="1,40,1,1,1" size="160"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="82" name="418/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="418" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="83" name="420/mean/Fused_Mul_1608516087_const" type="Const" version="opset1">
			<data element_type="f32" offset="124136" shape="120,40,1,1,1" size="19200"/>
			<output>
				<port id="0" names="backbone.features.5.conv.0.weight" precision="FP32">
					<dim>120</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="84" name="419" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="85" name="data_add_142731427814878/EltwiseUnsqueeze15345_const" type="Const" version="opset1">
			<data element_type="f32" offset="143336" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="86" name="420/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="420" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="87" name="421" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="421" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="88" name="2606326066_const" type="Const" version="opset1">
			<data element_type="f32" offset="143816" shape="120,1,1,1,5,5" size="12000"/>
			<output>
				<port id="0" names="backbone.features.5.conv.3.weight" precision="FP32">
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="89" name="422" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="90" name="data_add_142811428614880/EltwiseUnsqueeze15353_const" type="Const" version="opset1">
			<data element_type="f32" offset="155816" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="91" name="423/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="424,423" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="92" name="425" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,28,28" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="425" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="93" name="backbone.features.5.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="156296" shape="30,120,1,1,1" size="14400"/>
			<output>
				<port id="0" names="backbone.features.5.conv.5.fc.0.weight" precision="FP32">
					<dim>30</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="94" name="426/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>30</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="95" name="426/Dims840914817/EltwiseUnsqueeze15109_const" type="Const" version="opset1">
			<data element_type="f32" offset="170696" shape="1,30,1,1,1" size="120"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="96" name="426" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="426" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="97" name="427" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="427" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="98" name="backbone.features.5.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="170816" shape="120,30,1,1,1" size="14400"/>
			<output>
				<port id="0" names="backbone.features.5.conv.5.fc.2.weight" precision="FP32">
					<dim>120</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="99" name="428/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="100" name="data_add_142881429014881/EltwiseUnsqueeze15357_const" type="Const" version="opset1">
			<data element_type="f32" offset="185216" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="101" name="428/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="430" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="102" name="431" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="431" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="103" name="432/EltwiseUnsqueeze15121_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="432" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="104" name="433" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="433" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="105" name="434" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="434" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="106" name="435" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="435" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="107" name="437/mean/Fused_Mul_1609316095_const" type="Const" version="opset1">
			<data element_type="f32" offset="185696" shape="40,120,3,1,1" size="57600"/>
			<output>
				<port id="0" names="backbone.features.5.conv.7.weight" precision="FP32">
					<dim>40</dim>
					<dim>120</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="108" name="436" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>40</dim>
					<dim>120</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="109" name="data_add_142931429814883/EltwiseUnsqueeze15365_const" type="Const" version="opset1">
			<data element_type="f32" offset="243296" shape="1,40,1,1,1" size="160"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="110" name="437/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="437" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="111" name="438" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="438" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="112" name="440/mean/Fused_Mul_1609716099_const" type="Const" version="opset1">
			<data element_type="f32" offset="243456" shape="120,40,1,1,1" size="19200"/>
			<output>
				<port id="0" names="backbone.features.6.conv.0.weight" precision="FP32">
					<dim>120</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="113" name="439" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="114" name="data_add_143011430614885/EltwiseUnsqueeze15373_const" type="Const" version="opset1">
			<data element_type="f32" offset="262656" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="115" name="440/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="440" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="116" name="441" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="441" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="117" name="2611126114_const" type="Const" version="opset1">
			<data element_type="f32" offset="263136" shape="120,1,1,1,5,5" size="12000"/>
			<output>
				<port id="0" names="backbone.features.6.conv.3.weight" precision="FP32">
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="118" name="442" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="119" name="data_add_143091431414887/EltwiseUnsqueeze15381_const" type="Const" version="opset1">
			<data element_type="f32" offset="275136" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="120" name="443/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="444,443" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="121" name="445" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,28,28" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="445" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="122" name="backbone.features.6.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="275616" shape="30,120,1,1,1" size="14400"/>
			<output>
				<port id="0" names="backbone.features.6.conv.5.fc.0.weight" precision="FP32">
					<dim>30</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="123" name="446/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>30</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="124" name="446/Dims840314816/EltwiseUnsqueeze15105_const" type="Const" version="opset1">
			<data element_type="f32" offset="290016" shape="1,30,1,1,1" size="120"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="125" name="446" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="446" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="126" name="447" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="447" precision="FP32">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="127" name="backbone.features.6.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="290136" shape="120,30,1,1,1" size="14400"/>
			<output>
				<port id="0" names="backbone.features.6.conv.5.fc.2.weight" precision="FP32">
					<dim>120</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="128" name="448/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>30</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>30</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="129" name="data_add_143161431814888/EltwiseUnsqueeze15385_const" type="Const" version="opset1">
			<data element_type="f32" offset="304536" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="130" name="448/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="450" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="131" name="451" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="451" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="132" name="452/EltwiseUnsqueeze15125_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="452" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="133" name="453" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="453" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="134" name="454" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="454" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="135" name="455" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="455" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="136" name="457/mean/Fused_Mul_1610516107_const" type="Const" version="opset1">
			<data element_type="f32" offset="305016" shape="40,120,5,1,1" size="96000"/>
			<output>
				<port id="0" names="backbone.features.6.conv.7.weight" precision="FP32">
					<dim>40</dim>
					<dim>120</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="137" name="456" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="2,0,0" pads_end="2,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>40</dim>
					<dim>120</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="138" name="data_add_143211432614890/EltwiseUnsqueeze15393_const" type="Const" version="opset1">
			<data element_type="f32" offset="401016" shape="1,40,1,1,1" size="160"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="139" name="457/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="457" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="140" name="458" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="458" precision="FP32">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="141" name="460/mean/Fused_Mul_1610916111_const" type="Const" version="opset1">
			<data element_type="f32" offset="401176" shape="240,40,1,1,1" size="38400"/>
			<output>
				<port id="0" names="backbone.features.7.conv.0.weight" precision="FP32">
					<dim>240</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="142" name="459" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>40</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>240</dim>
					<dim>40</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="143" name="data_add_143291433414892/EltwiseUnsqueeze15401_const" type="Const" version="opset1">
			<data element_type="f32" offset="439576" shape="1,240,1,1,1" size="960"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="144" name="460/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="460" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="145" name="461/EltwiseUnsqueeze14977_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="461" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="146" name="462" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="462" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="147" name="463" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="1" names="463" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="148" name="464/EltwiseUnsqueeze15129_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="464" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="149" name="465" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="465" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="150" name="466" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</input>
			<output>
				<port id="2" names="466" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
			</output>
		</layer>
		<layer id="151" name="2609126094_const" type="Const" version="opset1">
			<data element_type="f32" offset="440536" shape="240,1,1,1,3,3" size="8640"/>
			<output>
				<port id="0" names="backbone.features.7.conv.3.weight" precision="FP32">
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="152" name="467" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,2,2"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>28</dim>
					<dim>28</dim>
				</port>
				<port id="1">
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="153" name="data_add_143371434214894/EltwiseUnsqueeze15409_const" type="Const" version="opset1">
			<data element_type="f32" offset="449176" shape="1,240,1,1,1" size="960"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="154" name="468/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="468" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="155" name="469/EltwiseUnsqueeze14981_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="469" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="156" name="470" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="470" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="157" name="471" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="471" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="158" name="472/EltwiseUnsqueeze15133_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="472" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="159" name="473" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="473" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="160" name="474" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="474" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="161" name="476/mean/Fused_Mul_1611716119_const" type="Const" version="opset1">
			<data element_type="f32" offset="450136" shape="80,240,5,1,1" size="384000"/>
			<output>
				<port id="0" names="backbone.features.7.conv.7.weight" precision="FP32">
					<dim>80</dim>
					<dim>240</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="162" name="475" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="2,0,0" pads_end="2,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>240</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="163" name="data_add_143451435014896/EltwiseUnsqueeze15417_const" type="Const" version="opset1">
			<data element_type="f32" offset="834136" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="164" name="476/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="476" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="165" name="478/mean/Fused_Mul_1612116123_const" type="Const" version="opset1">
			<data element_type="f32" offset="834456" shape="200,80,1,1,1" size="64000"/>
			<output>
				<port id="0" names="backbone.features.8.conv.0.weight" precision="FP32">
					<dim>200</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="166" name="477" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>200</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="167" name="data_add_143531435814898/EltwiseUnsqueeze15425_const" type="Const" version="opset1">
			<data element_type="f32" offset="898456" shape="1,200,1,1,1" size="800"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="168" name="478/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="478" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="169" name="479/EltwiseUnsqueeze14985_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="479" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="170" name="480" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="480" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="171" name="481" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="481" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="172" name="482/EltwiseUnsqueeze15137_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="482" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="173" name="483" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="483" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="174" name="484" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="484" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="175" name="2611926122_const" type="Const" version="opset1">
			<data element_type="f32" offset="899256" shape="200,1,1,1,3,3" size="7200"/>
			<output>
				<port id="0" names="backbone.features.8.conv.3.weight" precision="FP32">
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="176" name="485" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="177" name="data_add_143611436614900/EltwiseUnsqueeze15433_const" type="Const" version="opset1">
			<data element_type="f32" offset="906456" shape="1,200,1,1,1" size="800"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="178" name="486/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>200</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="486" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="179" name="487/EltwiseUnsqueeze14989_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="487" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="180" name="488" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="488" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="181" name="489" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="489" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="182" name="490/EltwiseUnsqueeze15141_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="490" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="183" name="491" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="491" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="184" name="492" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="492" precision="FP32">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="185" name="494/mean/Fused_Mul_1612916131_const" type="Const" version="opset1">
			<data element_type="f32" offset="907256" shape="80,200,3,1,1" size="192000"/>
			<output>
				<port id="0" names="backbone.features.8.conv.7.weight" precision="FP32">
					<dim>80</dim>
					<dim>200</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="186" name="493" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>200</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>200</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="187" name="data_add_143691437414902/EltwiseUnsqueeze15441_const" type="Const" version="opset1">
			<data element_type="f32" offset="1099256" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="188" name="494/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="494" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="189" name="495" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="495" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="190" name="497/mean/Fused_Mul_1613316135_const" type="Const" version="opset1">
			<data element_type="f32" offset="1099576" shape="184,80,1,1,1" size="58880"/>
			<output>
				<port id="0" names="backbone.features.9.conv.0.weight" precision="FP32">
					<dim>184</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="191" name="496" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>184</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="192" name="data_add_143771438214904/EltwiseUnsqueeze15449_const" type="Const" version="opset1">
			<data element_type="f32" offset="1158456" shape="1,184,1,1,1" size="736"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="193" name="497/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="497" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="194" name="498/EltwiseUnsqueeze14993_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="498" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="195" name="499" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="499" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="196" name="500" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="500" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="197" name="501/EltwiseUnsqueeze15145_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="501" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="198" name="502" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="502" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="199" name="503" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="503" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="200" name="2612726130_const" type="Const" version="opset1">
			<data element_type="f32" offset="1159192" shape="184,1,1,1,3,3" size="6624"/>
			<output>
				<port id="0" names="backbone.features.9.conv.3.weight" precision="FP32">
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="201" name="504" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="202" name="data_add_143851439014906/EltwiseUnsqueeze15457_const" type="Const" version="opset1">
			<data element_type="f32" offset="1165816" shape="1,184,1,1,1" size="736"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="203" name="505/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="505" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="204" name="506/EltwiseUnsqueeze14997_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="506" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="205" name="507" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="507" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="206" name="508" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="508" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="207" name="509/EltwiseUnsqueeze15149_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="509" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="208" name="510" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="510" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="209" name="511" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="511" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="210" name="513/mean/Fused_Mul_1614116143_const" type="Const" version="opset1">
			<data element_type="f32" offset="1166552" shape="80,184,3,1,1" size="176640"/>
			<output>
				<port id="0" names="backbone.features.9.conv.7.weight" precision="FP32">
					<dim>80</dim>
					<dim>184</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="211" name="512" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>184</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="212" name="data_add_143931439814908/EltwiseUnsqueeze15465_const" type="Const" version="opset1">
			<data element_type="f32" offset="1343192" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="213" name="513/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="513" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="214" name="514" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="514" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="215" name="516/mean/Fused_Mul_1614516147_const" type="Const" version="opset1">
			<data element_type="f32" offset="1343512" shape="184,80,1,1,1" size="58880"/>
			<output>
				<port id="0" names="backbone.features.10.conv.0.weight" precision="FP32">
					<dim>184</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="216" name="515" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>184</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="217" name="data_add_144011440614910/EltwiseUnsqueeze15473_const" type="Const" version="opset1">
			<data element_type="f32" offset="1402392" shape="1,184,1,1,1" size="736"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="218" name="516/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="516" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="219" name="517/EltwiseUnsqueeze15001_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="517" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="220" name="518" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="518" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="221" name="519" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="519" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="222" name="520/EltwiseUnsqueeze15153_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="520" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="223" name="521" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="521" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="224" name="522" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="522" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="225" name="2608726090_const" type="Const" version="opset1">
			<data element_type="f32" offset="1403128" shape="184,1,1,1,3,3" size="6624"/>
			<output>
				<port id="0" names="backbone.features.10.conv.3.weight" precision="FP32">
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="226" name="523" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="227" name="data_add_144091441414912/EltwiseUnsqueeze15481_const" type="Const" version="opset1">
			<data element_type="f32" offset="1409752" shape="1,184,1,1,1" size="736"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="228" name="524/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="524" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="229" name="525/EltwiseUnsqueeze15005_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="525" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="230" name="526" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="526" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="231" name="527" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="527" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="232" name="528/EltwiseUnsqueeze15157_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="528" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="233" name="529" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="529" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="234" name="530" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="530" precision="FP32">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="235" name="532/mean/Fused_Mul_1615316155_const" type="Const" version="opset1">
			<data element_type="f32" offset="1410488" shape="80,184,5,1,1" size="294400"/>
			<output>
				<port id="0" names="backbone.features.10.conv.7.weight" precision="FP32">
					<dim>80</dim>
					<dim>184</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="236" name="531" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="2,0,0" pads_end="2,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>184</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>184</dim>
					<dim>5</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="237" name="data_add_144171442214914/EltwiseUnsqueeze15489_const" type="Const" version="opset1">
			<data element_type="f32" offset="1704888" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="238" name="532/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="532" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="239" name="533" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="544,533" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="240" name="2607526078_const" type="Const" version="opset1">
			<data element_type="f32" offset="1705208" shape="80,1,1,1,3,3" size="2880"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_10.spatial_logits.0.weight" precision="FP32">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="241" name="534" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="242" name="data_add_144251443014916/EltwiseUnsqueeze15497_const" type="Const" version="opset1">
			<data element_type="f32" offset="1708088" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="243" name="535/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="535" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="244" name="536/EltwiseUnsqueeze15009_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="536" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="245" name="537" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="537" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="246" name="538" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="538" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="247" name="539/EltwiseUnsqueeze15161_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="539" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="248" name="540" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="540" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="249" name="541" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="541" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="250" name="543/mean/Fused_Mul_1616116163_const" type="Const" version="opset1">
			<data element_type="f32" offset="1708408" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_10.spatial_logits.3.weight" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="251" name="542" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="252" name="data_add_144331443814918/EltwiseUnsqueeze15505_const" type="Const" version="opset1">
			<data element_type="f32" offset="1708728" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="253" name="543/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="543" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="254" name="545" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,14,14" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="545" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="255" name="2608326086_const" type="Const" version="opset1">
			<data element_type="f32" offset="1708732" shape="80,1,1,3,1,1" size="960"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_10.temporal_logits.1.weight" precision="FP32">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="256" name="546" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="257" name="data_add_144411444614920/EltwiseUnsqueeze15513_const" type="Const" version="opset1">
			<data element_type="f32" offset="1709692" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="258" name="547/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="547" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="259" name="548/EltwiseUnsqueeze15013_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="548" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="260" name="549" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="549" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="261" name="550" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="550" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="262" name="551/EltwiseUnsqueeze15165_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="551" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="263" name="552" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="552" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="264" name="553" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="553" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="265" name="555/mean/Fused_Mul_1616916171_const" type="Const" version="opset1">
			<data element_type="f32" offset="1710012" shape="1,80,1,1,1" size="320"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_10.temporal_logits.4.weight" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="266" name="554" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="267" name="data_add_144491445414922/EltwiseUnsqueeze15521_const" type="Const" version="opset1">
			<data element_type="f32" offset="1710332" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="268" name="555/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="555" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="269" name="556" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="556" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="270" name="557" type="Sigmoid" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="557" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="271" name="558/EltwiseUnsqueeze15017_const" type="Const" version="opset1">
			<data element_type="f32" offset="1710336" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="558" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="272" name="559" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="559" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="273" name="560" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="560" precision="FP32">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="274" name="562/mean/Fused_Mul_1617316175_const" type="Const" version="opset1">
			<data element_type="f32" offset="1710340" shape="480,80,1,1,1" size="153600"/>
			<output>
				<port id="0" names="backbone.features.11.conv.0.weight" precision="FP32">
					<dim>480</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="275" name="561" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>80</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>480</dim>
					<dim>80</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="276" name="data_add_144571446214924/EltwiseUnsqueeze15529_const" type="Const" version="opset1">
			<data element_type="f32" offset="1863940" shape="1,480,1,1,1" size="1920"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="277" name="562/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="562" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="278" name="563/EltwiseUnsqueeze15021_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="563" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="279" name="564" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="564" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="280" name="565" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="565" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="281" name="566/EltwiseUnsqueeze15169_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="566" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="282" name="567" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="567" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="283" name="568" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="568" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="284" name="2612326126_const" type="Const" version="opset1">
			<data element_type="f32" offset="1865860" shape="480,1,1,1,3,3" size="17280"/>
			<output>
				<port id="0" names="backbone.features.11.conv.3.weight" precision="FP32">
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="285" name="569" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="286" name="data_add_144651447014926/EltwiseUnsqueeze15537_const" type="Const" version="opset1">
			<data element_type="f32" offset="1883140" shape="1,480,1,1,1" size="1920"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="287" name="570/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="571,570" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="288" name="572" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,14,14" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="572" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="289" name="backbone.features.11.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="1885060" shape="120,480,1,1,1" size="230400"/>
			<output>
				<port id="0" names="backbone.features.11.conv.5.fc.0.weight" precision="FP32">
					<dim>120</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="290" name="573/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>120</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="291" name="573/Dims836114812/EltwiseUnsqueeze15089_const" type="Const" version="opset1">
			<data element_type="f32" offset="2115460" shape="1,120,1,1,1" size="480"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="292" name="573" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="573" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="293" name="574" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="574" precision="FP32">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="294" name="backbone.features.11.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="2115940" shape="480,120,1,1,1" size="230400"/>
			<output>
				<port id="0" names="backbone.features.11.conv.5.fc.2.weight" precision="FP32">
					<dim>480</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="295" name="575/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>120</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>480</dim>
					<dim>120</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="296" name="data_add_144721447414927/EltwiseUnsqueeze15541_const" type="Const" version="opset1">
			<data element_type="f32" offset="2346340" shape="1,480,1,1,1" size="1920"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="297" name="575/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="577" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="298" name="578" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="578" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="299" name="579/EltwiseUnsqueeze15173_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="579" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="300" name="580" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="580" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="301" name="581" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="581" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="302" name="582/EltwiseUnsqueeze15025_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="582" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="303" name="583" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="583" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="304" name="584" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="584" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="305" name="585/EltwiseUnsqueeze15177_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="585" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="306" name="586" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="586" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="307" name="587" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="587" precision="FP32">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="308" name="589/mean/Fused_Mul_1618116183_const" type="Const" version="opset1">
			<data element_type="f32" offset="2348260" shape="112,480,3,1,1" size="645120"/>
			<output>
				<port id="0" names="backbone.features.11.conv.7.weight" precision="FP32">
					<dim>112</dim>
					<dim>480</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="309" name="588" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>480</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>112</dim>
					<dim>480</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="310" name="data_add_144771448214929/EltwiseUnsqueeze15549_const" type="Const" version="opset1">
			<data element_type="f32" offset="2993380" shape="1,112,1,1,1" size="448"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="311" name="589/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="590,589" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="312" name="591" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="2,1,1" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="2,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>8</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="591" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="313" name="593/mean/Fused_Mul_1618516187_const" type="Const" version="opset1">
			<data element_type="f32" offset="2993828" shape="672,112,1,1,1" size="301056"/>
			<output>
				<port id="0" names="backbone.features.12.conv.0.weight" precision="FP32">
					<dim>672</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="314" name="592" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="315" name="data_add_144851449014931/EltwiseUnsqueeze15557_const" type="Const" version="opset1">
			<data element_type="f32" offset="3294884" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="316" name="593/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="593" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="317" name="594/EltwiseUnsqueeze15029_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="594" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="318" name="595" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="595" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="319" name="596" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="596" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="320" name="597/EltwiseUnsqueeze15181_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="597" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="321" name="598" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="598" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="322" name="599" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="599" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="323" name="2607126074_const" type="Const" version="opset1">
			<data element_type="f32" offset="3297572" shape="672,1,1,1,3,3" size="24192"/>
			<output>
				<port id="0" names="backbone.features.12.conv.3.weight" precision="FP32">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="324" name="600" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="325" name="data_add_144931449814933/EltwiseUnsqueeze15565_const" type="Const" version="opset1">
			<data element_type="f32" offset="3321764" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="326" name="601/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="602,601" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="327" name="603" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,14,14" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="603" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="328" name="backbone.features.12.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="3324452" shape="168,672,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.12.conv.5.fc.0.weight" precision="FP32">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="329" name="604/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="330" name="604/Dims838514815/EltwiseUnsqueeze15101_const" type="Const" version="opset1">
			<data element_type="f32" offset="3776036" shape="1,168,1,1,1" size="672"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="331" name="604" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="604" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="332" name="605" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="605" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="333" name="backbone.features.12.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="3776708" shape="672,168,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.12.conv.5.fc.2.weight" precision="FP32">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="334" name="606/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="335" name="data_add_145001450214934/EltwiseUnsqueeze15569_const" type="Const" version="opset1">
			<data element_type="f32" offset="4228292" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="336" name="606/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="608" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="337" name="609" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="609" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="338" name="610/EltwiseUnsqueeze15185_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="610" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="339" name="611" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="611" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="340" name="612" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="612" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="341" name="613/EltwiseUnsqueeze15033_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="613" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="342" name="614" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="614" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="343" name="615" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="615" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="344" name="616/EltwiseUnsqueeze15189_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="616" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="345" name="617" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="617" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="346" name="618" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="618" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="347" name="620/mean/Fused_Mul_1619316195_const" type="Const" version="opset1">
			<data element_type="f32" offset="4230980" shape="112,672,3,1,1" size="903168"/>
			<output>
				<port id="0" names="backbone.features.12.conv.7.weight" precision="FP32">
					<dim>112</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="348" name="619" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>112</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="349" name="data_add_145051451014936/EltwiseUnsqueeze15577_const" type="Const" version="opset1">
			<data element_type="f32" offset="5134148" shape="1,112,1,1,1" size="448"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="350" name="620/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="620" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="351" name="621" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="621" precision="FP32">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="352" name="623/mean/Fused_Mul_1619716199_const" type="Const" version="opset1">
			<data element_type="f32" offset="5134596" shape="672,112,1,1,1" size="301056"/>
			<output>
				<port id="0" names="backbone.features.13.conv.0.weight" precision="FP32">
					<dim>672</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="353" name="622" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>112</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>112</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="354" name="data_add_145131451814938/EltwiseUnsqueeze15585_const" type="Const" version="opset1">
			<data element_type="f32" offset="5435652" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="355" name="623/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="623" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="356" name="624/EltwiseUnsqueeze15037_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="624" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="357" name="625" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="625" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="358" name="626" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="626" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="359" name="627/EltwiseUnsqueeze15193_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="627" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="360" name="628" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="628" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="361" name="629" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="629" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="362" name="2605926062_const" type="Const" version="opset1">
			<data element_type="f32" offset="5438340" shape="672,1,1,1,5,5" size="67200"/>
			<output>
				<port id="0" names="backbone.features.13.conv.3.weight" precision="FP32">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="363" name="630" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="364" name="data_add_145211452614940/EltwiseUnsqueeze15593_const" type="Const" version="opset1">
			<data element_type="f32" offset="5505540" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="365" name="631/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="632,631" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="366" name="633" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,14,14" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="633" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="367" name="backbone.features.13.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="5508228" shape="168,672,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.13.conv.5.fc.0.weight" precision="FP32">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="368" name="634/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="369" name="634/Dims837314814/EltwiseUnsqueeze15097_const" type="Const" version="opset1">
			<data element_type="f32" offset="5959812" shape="1,168,1,1,1" size="672"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="370" name="634" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="634" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="371" name="635" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="635" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="372" name="backbone.features.13.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="5960484" shape="672,168,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.13.conv.5.fc.2.weight" precision="FP32">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="373" name="636/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="374" name="data_add_145281453014941/EltwiseUnsqueeze15597_const" type="Const" version="opset1">
			<data element_type="f32" offset="6412068" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="375" name="636/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="638" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="376" name="639" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="639" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="377" name="640/EltwiseUnsqueeze15197_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="640" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="378" name="641" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="641" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="379" name="642" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="642" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="380" name="643/EltwiseUnsqueeze15041_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="643" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="381" name="644" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="644" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="382" name="645" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="645" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="383" name="646/EltwiseUnsqueeze15201_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="646" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="384" name="647" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="647" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="385" name="648" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="648" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="386" name="650/mean/Fused_Mul_1620516207_const" type="Const" version="opset1">
			<data element_type="f32" offset="6414756" shape="160,672,3,1,1" size="1290240"/>
			<output>
				<port id="0" names="backbone.features.13.conv.7.weight" precision="FP32">
					<dim>160</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="387" name="649" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>160</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="388" name="data_add_145331453814943/EltwiseUnsqueeze15605_const" type="Const" version="opset1">
			<data element_type="f32" offset="7704996" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="389" name="650/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="661,650" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="390" name="2609926102_const" type="Const" version="opset1">
			<data element_type="f32" offset="7705636" shape="160,1,1,1,3,3" size="5760"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_13.spatial_logits.0.weight" precision="FP32">
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</output>
		</layer>
		<layer id="391" name="651" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,1,1" pads_end="0,1,1" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>3</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="392" name="data_add_145411454614945/EltwiseUnsqueeze15613_const" type="Const" version="opset1">
			<data element_type="f32" offset="7711396" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="393" name="652/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="652" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="394" name="653/EltwiseUnsqueeze15045_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="653" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="395" name="654" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="654" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="396" name="655" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="655" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="397" name="656/EltwiseUnsqueeze15205_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="656" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="398" name="657" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="657" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="399" name="658" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="658" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="400" name="660/mean/Fused_Mul_1621316215_const" type="Const" version="opset1">
			<data element_type="f32" offset="7712036" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_13.spatial_logits.3.weight" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="401" name="659" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="402" name="data_add_145491455414947/EltwiseUnsqueeze15621_const" type="Const" version="opset1">
			<data element_type="f32" offset="7712676" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="403" name="660/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="660" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="404" name="662" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,14,14" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="662" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="405" name="2606726070_const" type="Const" version="opset1">
			<data element_type="f32" offset="7712680" shape="160,1,1,3,1,1" size="1920"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_13.temporal_logits.1.weight" precision="FP32">
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="406" name="663" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="407" name="data_add_145571456214949/EltwiseUnsqueeze15629_const" type="Const" version="opset1">
			<data element_type="f32" offset="7714600" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="408" name="664/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="664" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="409" name="665/EltwiseUnsqueeze15049_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="665" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="410" name="666" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="666" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="411" name="667" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="667" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="412" name="668/EltwiseUnsqueeze15209_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="668" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="413" name="669" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="669" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="414" name="670" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="670" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="415" name="672/mean/Fused_Mul_1622116223_const" type="Const" version="opset1">
			<data element_type="f32" offset="7715240" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" names="backbone.attentions.st_att_13.temporal_logits.4.weight" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="416" name="671" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="417" name="data_add_145651457014951/EltwiseUnsqueeze15637_const" type="Const" version="opset1">
			<data element_type="f32" offset="7715880" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="418" name="672/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="672" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="419" name="673" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="673" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="420" name="674" type="Sigmoid" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="674" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="421" name="675/EltwiseUnsqueeze15053_const" type="Const" version="opset1">
			<data element_type="f32" offset="1710336" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="675" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="422" name="676" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="676" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="423" name="677" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>1</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="677" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="424" name="679/mean/Fused_Mul_1622516227_const" type="Const" version="opset1">
			<data element_type="f32" offset="7715884" shape="672,160,1,1,1" size="430080"/>
			<output>
				<port id="0" names="backbone.features.14.conv.0.weight" precision="FP32">
					<dim>672</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="425" name="678" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="426" name="data_add_145731457814953/EltwiseUnsqueeze15645_const" type="Const" version="opset1">
			<data element_type="f32" offset="8145964" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="427" name="679/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="679" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="428" name="680/EltwiseUnsqueeze15057_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="680" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="429" name="681" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="681" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="430" name="682" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="1" names="682" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="431" name="683/EltwiseUnsqueeze15213_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="683" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="432" name="684" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="684" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="433" name="685" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</input>
			<output>
				<port id="2" names="685" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
			</output>
		</layer>
		<layer id="434" name="2613126134_const" type="Const" version="opset1">
			<data element_type="f32" offset="8148652" shape="672,1,1,1,5,5" size="67200"/>
			<output>
				<port id="0" names="backbone.features.14.conv.3.weight" precision="FP32">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="435" name="686" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,2,2"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>14</dim>
					<dim>14</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="436" name="data_add_145811458614955/EltwiseUnsqueeze15653_const" type="Const" version="opset1">
			<data element_type="f32" offset="8215852" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="437" name="687/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="688,687" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="438" name="689" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,7,7" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="689" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="439" name="backbone.features.14.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="8218540" shape="168,672,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.14.conv.5.fc.0.weight" precision="FP32">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="440" name="690/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>168</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="441" name="690/Dims834914810/EltwiseUnsqueeze15081_const" type="Const" version="opset1">
			<data element_type="f32" offset="8670124" shape="1,168,1,1,1" size="672"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="442" name="690" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="690" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="443" name="691" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="691" precision="FP32">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="444" name="backbone.features.14.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="8670796" shape="672,168,1,1,1" size="451584"/>
			<output>
				<port id="0" names="backbone.features.14.conv.5.fc.2.weight" precision="FP32">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="445" name="692/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>168</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>672</dim>
					<dim>168</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="446" name="data_add_145881459014956/EltwiseUnsqueeze15657_const" type="Const" version="opset1">
			<data element_type="f32" offset="9122380" shape="1,672,1,1,1" size="2688"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="447" name="692/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="694" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="448" name="695" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="695" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="449" name="696/EltwiseUnsqueeze15217_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="696" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="450" name="697" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="697" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="451" name="698" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="698" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="452" name="699/EltwiseUnsqueeze15061_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="699" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="453" name="700" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="700" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="454" name="701" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="701" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="455" name="702/EltwiseUnsqueeze15221_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="702" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="456" name="703" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="703" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="457" name="704" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="704" precision="FP32">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="458" name="706/mean/Fused_Mul_1623316235_const" type="Const" version="opset1">
			<data element_type="f32" offset="9125068" shape="160,672,3,1,1" size="1290240"/>
			<output>
				<port id="0" names="backbone.features.14.conv.7.weight" precision="FP32">
					<dim>160</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="459" name="705" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>672</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>160</dim>
					<dim>672</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="460" name="data_add_145931459814958/EltwiseUnsqueeze15665_const" type="Const" version="opset1">
			<data element_type="f32" offset="10415308" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="461" name="706/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="706" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="462" name="708/mean/Fused_Mul_1623716239_const" type="Const" version="opset1">
			<data element_type="f32" offset="10415948" shape="960,160,1,1,1" size="614400"/>
			<output>
				<port id="0" names="backbone.features.15.conv.0.weight" precision="FP32">
					<dim>960</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="463" name="707" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>960</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="464" name="data_add_146011460614960/EltwiseUnsqueeze15673_const" type="Const" version="opset1">
			<data element_type="f32" offset="11030348" shape="1,960,1,1,1" size="3840"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="465" name="708/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="708" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="466" name="709/EltwiseUnsqueeze15065_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="709" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="467" name="710" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="710" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="468" name="711" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="711" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="469" name="712/EltwiseUnsqueeze15225_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="712" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="470" name="713" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="713" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="471" name="714" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="714" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="472" name="2611526118_const" type="Const" version="opset1">
			<data element_type="f32" offset="11034188" shape="960,1,1,1,5,5" size="96000"/>
			<output>
				<port id="0" names="backbone.features.15.conv.3.weight" precision="FP32">
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</output>
		</layer>
		<layer id="473" name="715" type="GroupConvolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,2,2" pads_end="0,2,2" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>5</dim>
					<dim>5</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="474" name="data_add_146091461414962/EltwiseUnsqueeze15681_const" type="Const" version="opset1">
			<data element_type="f32" offset="11130188" shape="1,960,1,1,1" size="3840"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="475" name="716/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="717,716" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="476" name="718" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="1,7,7" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="718" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="477" name="backbone.features.15.conv.5.fc.0.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="11134028" shape="240,960,1,1,1" size="921600"/>
			<output>
				<port id="0" names="backbone.features.15.conv.5.fc.0.weight" precision="FP32">
					<dim>240</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="478" name="719/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>240</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="479" name="719/Dims836714813/EltwiseUnsqueeze15093_const" type="Const" version="opset1">
			<data element_type="f32" offset="12055628" shape="1,240,1,1,1" size="960"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="480" name="719" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="719" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="481" name="720" type="ReLU" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="720" precision="FP32">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="482" name="backbone.features.15.conv.5.fc.2.weight/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="f32" offset="12056588" shape="960,240,1,1,1" size="921600"/>
			<output>
				<port id="0" names="backbone.features.15.conv.5.fc.2.weight" precision="FP32">
					<dim>960</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="483" name="721/WithoutBiases" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>240</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>960</dim>
					<dim>240</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="484" name="data_add_146161461814963/EltwiseUnsqueeze15685_const" type="Const" version="opset1">
			<data element_type="f32" offset="12978188" shape="1,960,1,1,1" size="3840"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="485" name="721/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="723" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="486" name="724" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="1" names="724" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="487" name="725/EltwiseUnsqueeze15229_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="725" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="488" name="726" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="726" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="489" name="727" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="727" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="490" name="728/EltwiseUnsqueeze15069_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="728" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="491" name="729" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="729" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="492" name="730" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="730" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="493" name="731/EltwiseUnsqueeze15233_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="731" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="494" name="732" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="732" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="495" name="733" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="733" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="496" name="735/mean/Fused_Mul_1624516247_const" type="Const" version="opset1">
			<data element_type="f32" offset="12982028" shape="160,960,3,1,1" size="1843200"/>
			<output>
				<port id="0" names="backbone.features.15.conv.7.weight" precision="FP32">
					<dim>160</dim>
					<dim>960</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="497" name="734" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="1,0,0" pads_end="1,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>160</dim>
					<dim>960</dim>
					<dim>3</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="498" name="data_add_146211462614965/EltwiseUnsqueeze15693_const" type="Const" version="opset1">
			<data element_type="f32" offset="14825228" shape="1,160,1,1,1" size="640"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="499" name="735/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="735" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="500" name="736" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="736" precision="FP32">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="501" name="738/mean/Fused_Mul_1624916251_const" type="Const" version="opset1">
			<data element_type="f32" offset="14825868" shape="960,160,1,1,1" size="614400"/>
			<output>
				<port id="0" names="backbone.conv.0.0.weight" precision="FP32">
					<dim>960</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="502" name="737" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>160</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>960</dim>
					<dim>160</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="503" name="data_add_146291463414967/EltwiseUnsqueeze15701_const" type="Const" version="opset1">
			<data element_type="f32" offset="15440268" shape="1,960,1,1,1" size="3840"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="504" name="738/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="738" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="505" name="739/EltwiseUnsqueeze15073_const" type="Const" version="opset1">
			<data element_type="f32" offset="1816" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="739" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="506" name="740" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="740" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="507" name="741" type="Clamp" version="opset1">
			<data max="6.0" min="0.0"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="741" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="508" name="742/EltwiseUnsqueeze15237_const" type="Const" version="opset1">
			<data element_type="f32" offset="1820" shape="1,1,1,1,1" size="4"/>
			<output>
				<port id="0" names="742" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="509" name="743" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="743" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="510" name="744" type="Multiply" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="2" names="745,744" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</output>
		</layer>
		<layer id="511" name="746" type="AvgPool" version="opset1">
			<data auto_pad="explicit" exclude-pad="true" kernel="4,7,7" pads_begin="0,0,0" pads_end="0,0,0" rounding_type="floor" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>4</dim>
					<dim>7</dim>
					<dim>7</dim>
				</port>
			</input>
			<output>
				<port id="1" names="746" precision="FP32">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="512" name="748/mean/Fused_Mul_1625316255_const" type="Const" version="opset1">
			<data element_type="f32" offset="15444108" shape="256,960,1,1,1" size="983040"/>
			<output>
				<port id="0" names="cls_head.fc_pre_angular.0.weight" precision="FP32">
					<dim>256</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="513" name="747" type="Convolution" version="opset1">
			<data auto_pad="explicit" dilations="1,1,1" pads_begin="0,0,0" pads_end="0,0,0" strides="1,1,1"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>256</dim>
					<dim>960</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="514" name="data_add_146371464214969/EltwiseUnsqueeze15709_const" type="Const" version="opset1">
			<data element_type="f32" offset="16427148" shape="1,256,1,1,1" size="1024"/>
			<output>
				<port id="0" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="515" name="748/variance/Fused_Add_" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="748" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="516" name="750/Cast_126556_const" type="Const" version="opset1">
			<data element_type="i64" offset="16428172" shape="2" size="16"/>
			<output>
				<port id="0" names="749" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="517" name="750" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>256</dim>
					<dim>1</dim>
					<dim>1</dim>
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" names="750" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="518" name="751/Axis/Output_0/Data__const" type="Const" version="opset1">
			<data element_type="i64" offset="16428188" shape="1" size="8"/>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="519" name="751" type="NormalizeL2" version="opset1">
			<data eps="1e-06" eps_mode="add"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="751" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="520" name="755/MinusOne20605_const" type="Const" version="opset1">
			<data element_type="i64" offset="16428196" shape="1" size="8"/>
			<output>
				<port id="0" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="521" name="output759/WithoutBiases/1_port_transpose20324_const" type="Const" version="opset1">
			<data element_type="f32" offset="16428204" shape="100,256" size="102400"/>
			<output>
				<port id="0" names="755" precision="FP32">
					<dim>100</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="522" name="755/Shape" type="ShapeOf" version="opset3">
			<data output_type="i64"/>
			<input>
				<port id="0">
					<dim>100</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="1" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="523" name="755/Shape/Gather/Cast_126558_const" type="Const" version="opset1">
			<data element_type="i32" offset="16530604" shape="1" size="4"/>
			<output>
				<port id="0" precision="I32">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="524" name="755/Shape/Gather/Cast_226560_const" type="Const" version="opset1">
			<data element_type="i64" offset="16530608" shape="" size="8"/>
			<output>
				<port id="0" precision="I64"/>
			</output>
		</layer>
		<layer id="525" name="755/Shape/Gather" type="Gather" version="opset1">
			<input>
				<port id="0">
					<dim>2</dim>
				</port>
				<port id="1">
					<dim>1</dim>
				</port>
				<port id="2"/>
			</input>
			<output>
				<port id="3" precision="I64">
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="526" name="755/MinusOne/shapes_concat" type="Concat" version="opset1">
			<data axis="0"/>
			<input>
				<port id="0">
					<dim>1</dim>
				</port>
				<port id="1">
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="752" precision="I64">
					<dim>2</dim>
				</port>
			</output>
		</layer>
		<layer id="527" name="753" type="Reshape" version="opset1">
			<data special_zero="true"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1">
					<dim>2</dim>
				</port>
			</input>
			<output>
				<port id="2" names="753" precision="FP32">
					<dim>1</dim>
					<dim>256</dim>
				</port>
			</output>
		</layer>
		<layer id="528" name="output759/WithoutBiases" type="MatMul" version="opset1">
			<data transpose_a="false" transpose_b="true"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>256</dim>
				</port>
				<port id="1">
					<dim>100</dim>
					<dim>256</dim>
				</port>
			</input>
			<output>
				<port id="2" precision="FP32">
					<dim>1</dim>
					<dim>100</dim>
				</port>
			</output>
		</layer>
		<layer id="529" name="output759/Beta_/EltwiseUnsqueeze15077_const" type="Const" version="opset1">
			<data element_type="f32" offset="16530616" shape="1,1" size="4"/>
			<output>
				<port id="0" names="756" precision="FP32">
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</output>
		</layer>
		<layer id="530" name="output" type="Add" version="opset1">
			<data auto_broadcast="numpy"/>
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>100</dim>
				</port>
				<port id="1">
					<dim>1</dim>
					<dim>1</dim>
				</port>
			</input>
			<output>
				<port id="2" names="output" precision="FP32">
					<dim>1</dim>
					<dim>100</dim>
				</port>
			</output>
		</layer>
		<layer id="531" name="output/sink_port_0" type="Result" version="opset1">
			<input>
				<port id="0">
					<dim>1</dim>
					<dim>100</dim>
				</port>
			</input>
		</layer>
	</layers>
	<edges>
		<edge from-layer="0" from-port="0" to-layer="2" to-port="0"/>
		<edge from-layer="1" from-port="0" to-layer="2" to-port="1"/>
		<edge from-layer="2" from-port="2" to-layer="4" to-port="0"/>
		<edge from-layer="3" from-port="0" to-layer="4" to-port="1"/>
		<edge from-layer="4" from-port="2" to-layer="6" to-port="0"/>
		<edge from-layer="5" from-port="0" to-layer="6" to-port="1"/>
		<edge from-layer="6" from-port="2" to-layer="8" to-port="0"/>
		<edge from-layer="7" from-port="0" to-layer="8" to-port="1"/>
		<edge from-layer="8" from-port="2" to-layer="10" to-port="0"/>
		<edge from-layer="9" from-port="0" to-layer="10" to-port="1"/>
		<edge from-layer="10" from-port="2" to-layer="11" to-port="0"/>
		<edge from-layer="11" from-port="1" to-layer="13" to-port="0"/>
		<edge from-layer="12" from-port="0" to-layer="13" to-port="1"/>
		<edge from-layer="8" from-port="2" to-layer="14" to-port="0"/>
		<edge from-layer="13" from-port="2" to-layer="14" to-port="1"/>
		<edge from-layer="14" from-port="2" to-layer="16" to-port="0"/>
		<edge from-layer="15" from-port="0" to-layer="16" to-port="1"/>
		<edge from-layer="16" from-port="2" to-layer="18" to-port="0"/>
		<edge from-layer="17" from-port="0" to-layer="18" to-port="1"/>
		<edge from-layer="18" from-port="2" to-layer="19" to-port="0"/>
		<edge from-layer="19" from-port="1" to-layer="21" to-port="0"/>
		<edge from-layer="20" from-port="0" to-layer="21" to-port="1"/>
		<edge from-layer="21" from-port="2" to-layer="23" to-port="0"/>
		<edge from-layer="22" from-port="0" to-layer="23" to-port="1"/>
		<edge from-layer="14" from-port="2" to-layer="24" to-port="0"/>
		<edge from-layer="23" from-port="2" to-layer="24" to-port="1"/>
		<edge from-layer="24" from-port="2" to-layer="26" to-port="0"/>
		<edge from-layer="25" from-port="0" to-layer="26" to-port="1"/>
		<edge from-layer="26" from-port="2" to-layer="28" to-port="0"/>
		<edge from-layer="27" from-port="0" to-layer="28" to-port="1"/>
		<edge from-layer="28" from-port="2" to-layer="29" to-port="0"/>
		<edge from-layer="29" from-port="1" to-layer="31" to-port="0"/>
		<edge from-layer="30" from-port="0" to-layer="31" to-port="1"/>
		<edge from-layer="31" from-port="2" to-layer="33" to-port="0"/>
		<edge from-layer="32" from-port="0" to-layer="33" to-port="1"/>
		<edge from-layer="33" from-port="2" to-layer="34" to-port="0"/>
		<edge from-layer="34" from-port="1" to-layer="36" to-port="0"/>
		<edge from-layer="35" from-port="0" to-layer="36" to-port="1"/>
		<edge from-layer="36" from-port="2" to-layer="38" to-port="0"/>
		<edge from-layer="37" from-port="0" to-layer="38" to-port="1"/>
		<edge from-layer="38" from-port="2" to-layer="39" to-port="0"/>
		<edge from-layer="39" from-port="1" to-layer="41" to-port="0"/>
		<edge from-layer="40" from-port="0" to-layer="41" to-port="1"/>
		<edge from-layer="41" from-port="2" to-layer="43" to-port="0"/>
		<edge from-layer="42" from-port="0" to-layer="43" to-port="1"/>
		<edge from-layer="43" from-port="2" to-layer="44" to-port="0"/>
		<edge from-layer="44" from-port="1" to-layer="46" to-port="0"/>
		<edge from-layer="45" from-port="0" to-layer="46" to-port="1"/>
		<edge from-layer="46" from-port="2" to-layer="48" to-port="0"/>
		<edge from-layer="47" from-port="0" to-layer="48" to-port="1"/>
		<edge from-layer="48" from-port="2" to-layer="49" to-port="0"/>
		<edge from-layer="49" from-port="1" to-layer="51" to-port="0"/>
		<edge from-layer="50" from-port="0" to-layer="51" to-port="1"/>
		<edge from-layer="51" from-port="2" to-layer="53" to-port="0"/>
		<edge from-layer="52" from-port="0" to-layer="53" to-port="1"/>
		<edge from-layer="39" from-port="1" to-layer="54" to-port="0"/>
		<edge from-layer="53" from-port="2" to-layer="54" to-port="1"/>
		<edge from-layer="54" from-port="2" to-layer="56" to-port="0"/>
		<edge from-layer="55" from-port="0" to-layer="56" to-port="1"/>
		<edge from-layer="56" from-port="2" to-layer="58" to-port="0"/>
		<edge from-layer="57" from-port="0" to-layer="58" to-port="1"/>
		<edge from-layer="58" from-port="2" to-layer="59" to-port="0"/>
		<edge from-layer="59" from-port="1" to-layer="61" to-port="0"/>
		<edge from-layer="60" from-port="0" to-layer="61" to-port="1"/>
		<edge from-layer="61" from-port="2" to-layer="63" to-port="0"/>
		<edge from-layer="62" from-port="0" to-layer="63" to-port="1"/>
		<edge from-layer="63" from-port="2" to-layer="64" to-port="0"/>
		<edge from-layer="64" from-port="1" to-layer="66" to-port="0"/>
		<edge from-layer="65" from-port="0" to-layer="66" to-port="1"/>
		<edge from-layer="66" from-port="2" to-layer="68" to-port="0"/>
		<edge from-layer="67" from-port="0" to-layer="68" to-port="1"/>
		<edge from-layer="68" from-port="2" to-layer="69" to-port="0"/>
		<edge from-layer="69" from-port="1" to-layer="71" to-port="0"/>
		<edge from-layer="70" from-port="0" to-layer="71" to-port="1"/>
		<edge from-layer="71" from-port="2" to-layer="73" to-port="0"/>
		<edge from-layer="72" from-port="0" to-layer="73" to-port="1"/>
		<edge from-layer="73" from-port="2" to-layer="74" to-port="0"/>
		<edge from-layer="74" from-port="1" to-layer="76" to-port="0"/>
		<edge from-layer="75" from-port="0" to-layer="76" to-port="1"/>
		<edge from-layer="76" from-port="2" to-layer="77" to-port="0"/>
		<edge from-layer="63" from-port="2" to-layer="77" to-port="1"/>
		<edge from-layer="77" from-port="2" to-layer="78" to-port="0"/>
		<edge from-layer="78" from-port="1" to-layer="80" to-port="0"/>
		<edge from-layer="79" from-port="0" to-layer="80" to-port="1"/>
		<edge from-layer="80" from-port="2" to-layer="82" to-port="0"/>
		<edge from-layer="81" from-port="0" to-layer="82" to-port="1"/>
		<edge from-layer="82" from-port="2" to-layer="84" to-port="0"/>
		<edge from-layer="83" from-port="0" to-layer="84" to-port="1"/>
		<edge from-layer="84" from-port="2" to-layer="86" to-port="0"/>
		<edge from-layer="85" from-port="0" to-layer="86" to-port="1"/>
		<edge from-layer="86" from-port="2" to-layer="87" to-port="0"/>
		<edge from-layer="87" from-port="1" to-layer="89" to-port="0"/>
		<edge from-layer="88" from-port="0" to-layer="89" to-port="1"/>
		<edge from-layer="89" from-port="2" to-layer="91" to-port="0"/>
		<edge from-layer="90" from-port="0" to-layer="91" to-port="1"/>
		<edge from-layer="91" from-port="2" to-layer="92" to-port="0"/>
		<edge from-layer="92" from-port="1" to-layer="94" to-port="0"/>
		<edge from-layer="93" from-port="0" to-layer="94" to-port="1"/>
		<edge from-layer="94" from-port="2" to-layer="96" to-port="0"/>
		<edge from-layer="95" from-port="0" to-layer="96" to-port="1"/>
		<edge from-layer="96" from-port="2" to-layer="97" to-port="0"/>
		<edge from-layer="97" from-port="1" to-layer="99" to-port="0"/>
		<edge from-layer="98" from-port="0" to-layer="99" to-port="1"/>
		<edge from-layer="99" from-port="2" to-layer="101" to-port="0"/>
		<edge from-layer="100" from-port="0" to-layer="101" to-port="1"/>
		<edge from-layer="101" from-port="2" to-layer="102" to-port="0"/>
		<edge from-layer="102" from-port="1" to-layer="104" to-port="0"/>
		<edge from-layer="103" from-port="0" to-layer="104" to-port="1"/>
		<edge from-layer="104" from-port="2" to-layer="105" to-port="0"/>
		<edge from-layer="91" from-port="2" to-layer="105" to-port="1"/>
		<edge from-layer="105" from-port="2" to-layer="106" to-port="0"/>
		<edge from-layer="106" from-port="1" to-layer="108" to-port="0"/>
		<edge from-layer="107" from-port="0" to-layer="108" to-port="1"/>
		<edge from-layer="108" from-port="2" to-layer="110" to-port="0"/>
		<edge from-layer="109" from-port="0" to-layer="110" to-port="1"/>
		<edge from-layer="82" from-port="2" to-layer="111" to-port="0"/>
		<edge from-layer="110" from-port="2" to-layer="111" to-port="1"/>
		<edge from-layer="111" from-port="2" to-layer="113" to-port="0"/>
		<edge from-layer="112" from-port="0" to-layer="113" to-port="1"/>
		<edge from-layer="113" from-port="2" to-layer="115" to-port="0"/>
		<edge from-layer="114" from-port="0" to-layer="115" to-port="1"/>
		<edge from-layer="115" from-port="2" to-layer="116" to-port="0"/>
		<edge from-layer="116" from-port="1" to-layer="118" to-port="0"/>
		<edge from-layer="117" from-port="0" to-layer="118" to-port="1"/>
		<edge from-layer="118" from-port="2" to-layer="120" to-port="0"/>
		<edge from-layer="119" from-port="0" to-layer="120" to-port="1"/>
		<edge from-layer="120" from-port="2" to-layer="121" to-port="0"/>
		<edge from-layer="121" from-port="1" to-layer="123" to-port="0"/>
		<edge from-layer="122" from-port="0" to-layer="123" to-port="1"/>
		<edge from-layer="123" from-port="2" to-layer="125" to-port="0"/>
		<edge from-layer="124" from-port="0" to-layer="125" to-port="1"/>
		<edge from-layer="125" from-port="2" to-layer="126" to-port="0"/>
		<edge from-layer="126" from-port="1" to-layer="128" to-port="0"/>
		<edge from-layer="127" from-port="0" to-layer="128" to-port="1"/>
		<edge from-layer="128" from-port="2" to-layer="130" to-port="0"/>
		<edge from-layer="129" from-port="0" to-layer="130" to-port="1"/>
		<edge from-layer="130" from-port="2" to-layer="131" to-port="0"/>
		<edge from-layer="131" from-port="1" to-layer="133" to-port="0"/>
		<edge from-layer="132" from-port="0" to-layer="133" to-port="1"/>
		<edge from-layer="133" from-port="2" to-layer="134" to-port="0"/>
		<edge from-layer="120" from-port="2" to-layer="134" to-port="1"/>
		<edge from-layer="134" from-port="2" to-layer="135" to-port="0"/>
		<edge from-layer="135" from-port="1" to-layer="137" to-port="0"/>
		<edge from-layer="136" from-port="0" to-layer="137" to-port="1"/>
		<edge from-layer="137" from-port="2" to-layer="139" to-port="0"/>
		<edge from-layer="138" from-port="0" to-layer="139" to-port="1"/>
		<edge from-layer="111" from-port="2" to-layer="140" to-port="0"/>
		<edge from-layer="139" from-port="2" to-layer="140" to-port="1"/>
		<edge from-layer="140" from-port="2" to-layer="142" to-port="0"/>
		<edge from-layer="141" from-port="0" to-layer="142" to-port="1"/>
		<edge from-layer="142" from-port="2" to-layer="144" to-port="0"/>
		<edge from-layer="143" from-port="0" to-layer="144" to-port="1"/>
		<edge from-layer="144" from-port="2" to-layer="146" to-port="0"/>
		<edge from-layer="145" from-port="0" to-layer="146" to-port="1"/>
		<edge from-layer="146" from-port="2" to-layer="147" to-port="0"/>
		<edge from-layer="147" from-port="1" to-layer="149" to-port="0"/>
		<edge from-layer="148" from-port="0" to-layer="149" to-port="1"/>
		<edge from-layer="144" from-port="2" to-layer="150" to-port="0"/>
		<edge from-layer="149" from-port="2" to-layer="150" to-port="1"/>
		<edge from-layer="150" from-port="2" to-layer="152" to-port="0"/>
		<edge from-layer="151" from-port="0" to-layer="152" to-port="1"/>
		<edge from-layer="152" from-port="2" to-layer="154" to-port="0"/>
		<edge from-layer="153" from-port="0" to-layer="154" to-port="1"/>
		<edge from-layer="154" from-port="2" to-layer="156" to-port="0"/>
		<edge from-layer="155" from-port="0" to-layer="156" to-port="1"/>
		<edge from-layer="156" from-port="2" to-layer="157" to-port="0"/>
		<edge from-layer="157" from-port="1" to-layer="159" to-port="0"/>
		<edge from-layer="158" from-port="0" to-layer="159" to-port="1"/>
		<edge from-layer="154" from-port="2" to-layer="160" to-port="0"/>
		<edge from-layer="159" from-port="2" to-layer="160" to-port="1"/>
		<edge from-layer="160" from-port="2" to-layer="162" to-port="0"/>
		<edge from-layer="161" from-port="0" to-layer="162" to-port="1"/>
		<edge from-layer="162" from-port="2" to-layer="164" to-port="0"/>
		<edge from-layer="163" from-port="0" to-layer="164" to-port="1"/>
		<edge from-layer="164" from-port="2" to-layer="166" to-port="0"/>
		<edge from-layer="165" from-port="0" to-layer="166" to-port="1"/>
		<edge from-layer="166" from-port="2" to-layer="168" to-port="0"/>
		<edge from-layer="167" from-port="0" to-layer="168" to-port="1"/>
		<edge from-layer="168" from-port="2" to-layer="170" to-port="0"/>
		<edge from-layer="169" from-port="0" to-layer="170" to-port="1"/>
		<edge from-layer="170" from-port="2" to-layer="171" to-port="0"/>
		<edge from-layer="171" from-port="1" to-layer="173" to-port="0"/>
		<edge from-layer="172" from-port="0" to-layer="173" to-port="1"/>
		<edge from-layer="168" from-port="2" to-layer="174" to-port="0"/>
		<edge from-layer="173" from-port="2" to-layer="174" to-port="1"/>
		<edge from-layer="174" from-port="2" to-layer="176" to-port="0"/>
		<edge from-layer="175" from-port="0" to-layer="176" to-port="1"/>
		<edge from-layer="176" from-port="2" to-layer="178" to-port="0"/>
		<edge from-layer="177" from-port="0" to-layer="178" to-port="1"/>
		<edge from-layer="178" from-port="2" to-layer="180" to-port="0"/>
		<edge from-layer="179" from-port="0" to-layer="180" to-port="1"/>
		<edge from-layer="180" from-port="2" to-layer="181" to-port="0"/>
		<edge from-layer="181" from-port="1" to-layer="183" to-port="0"/>
		<edge from-layer="182" from-port="0" to-layer="183" to-port="1"/>
		<edge from-layer="178" from-port="2" to-layer="184" to-port="0"/>
		<edge from-layer="183" from-port="2" to-layer="184" to-port="1"/>
		<edge from-layer="184" from-port="2" to-layer="186" to-port="0"/>
		<edge from-layer="185" from-port="0" to-layer="186" to-port="1"/>
		<edge from-layer="186" from-port="2" to-layer="188" to-port="0"/>
		<edge from-layer="187" from-port="0" to-layer="188" to-port="1"/>
		<edge from-layer="164" from-port="2" to-layer="189" to-port="0"/>
		<edge from-layer="188" from-port="2" to-layer="189" to-port="1"/>
		<edge from-layer="189" from-port="2" to-layer="191" to-port="0"/>
		<edge from-layer="190" from-port="0" to-layer="191" to-port="1"/>
		<edge from-layer="191" from-port="2" to-layer="193" to-port="0"/>
		<edge from-layer="192" from-port="0" to-layer="193" to-port="1"/>
		<edge from-layer="193" from-port="2" to-layer="195" to-port="0"/>
		<edge from-layer="194" from-port="0" to-layer="195" to-port="1"/>
		<edge from-layer="195" from-port="2" to-layer="196" to-port="0"/>
		<edge from-layer="196" from-port="1" to-layer="198" to-port="0"/>
		<edge from-layer="197" from-port="0" to-layer="198" to-port="1"/>
		<edge from-layer="193" from-port="2" to-layer="199" to-port="0"/>
		<edge from-layer="198" from-port="2" to-layer="199" to-port="1"/>
		<edge from-layer="199" from-port="2" to-layer="201" to-port="0"/>
		<edge from-layer="200" from-port="0" to-layer="201" to-port="1"/>
		<edge from-layer="201" from-port="2" to-layer="203" to-port="0"/>
		<edge from-layer="202" from-port="0" to-layer="203" to-port="1"/>
		<edge from-layer="203" from-port="2" to-layer="205" to-port="0"/>
		<edge from-layer="204" from-port="0" to-layer="205" to-port="1"/>
		<edge from-layer="205" from-port="2" to-layer="206" to-port="0"/>
		<edge from-layer="206" from-port="1" to-layer="208" to-port="0"/>
		<edge from-layer="207" from-port="0" to-layer="208" to-port="1"/>
		<edge from-layer="203" from-port="2" to-layer="209" to-port="0"/>
		<edge from-layer="208" from-port="2" to-layer="209" to-port="1"/>
		<edge from-layer="209" from-port="2" to-layer="211" to-port="0"/>
		<edge from-layer="210" from-port="0" to-layer="211" to-port="1"/>
		<edge from-layer="211" from-port="2" to-layer="213" to-port="0"/>
		<edge from-layer="212" from-port="0" to-layer="213" to-port="1"/>
		<edge from-layer="189" from-port="2" to-layer="214" to-port="0"/>
		<edge from-layer="213" from-port="2" to-layer="214" to-port="1"/>
		<edge from-layer="214" from-port="2" to-layer="216" to-port="0"/>
		<edge from-layer="215" from-port="0" to-layer="216" to-port="1"/>
		<edge from-layer="216" from-port="2" to-layer="218" to-port="0"/>
		<edge from-layer="217" from-port="0" to-layer="218" to-port="1"/>
		<edge from-layer="218" from-port="2" to-layer="220" to-port="0"/>
		<edge from-layer="219" from-port="0" to-layer="220" to-port="1"/>
		<edge from-layer="220" from-port="2" to-layer="221" to-port="0"/>
		<edge from-layer="221" from-port="1" to-layer="223" to-port="0"/>
		<edge from-layer="222" from-port="0" to-layer="223" to-port="1"/>
		<edge from-layer="218" from-port="2" to-layer="224" to-port="0"/>
		<edge from-layer="223" from-port="2" to-layer="224" to-port="1"/>
		<edge from-layer="224" from-port="2" to-layer="226" to-port="0"/>
		<edge from-layer="225" from-port="0" to-layer="226" to-port="1"/>
		<edge from-layer="226" from-port="2" to-layer="228" to-port="0"/>
		<edge from-layer="227" from-port="0" to-layer="228" to-port="1"/>
		<edge from-layer="228" from-port="2" to-layer="230" to-port="0"/>
		<edge from-layer="229" from-port="0" to-layer="230" to-port="1"/>
		<edge from-layer="230" from-port="2" to-layer="231" to-port="0"/>
		<edge from-layer="231" from-port="1" to-layer="233" to-port="0"/>
		<edge from-layer="232" from-port="0" to-layer="233" to-port="1"/>
		<edge from-layer="228" from-port="2" to-layer="234" to-port="0"/>
		<edge from-layer="233" from-port="2" to-layer="234" to-port="1"/>
		<edge from-layer="234" from-port="2" to-layer="236" to-port="0"/>
		<edge from-layer="235" from-port="0" to-layer="236" to-port="1"/>
		<edge from-layer="236" from-port="2" to-layer="238" to-port="0"/>
		<edge from-layer="237" from-port="0" to-layer="238" to-port="1"/>
		<edge from-layer="214" from-port="2" to-layer="239" to-port="0"/>
		<edge from-layer="238" from-port="2" to-layer="239" to-port="1"/>
		<edge from-layer="239" from-port="2" to-layer="241" to-port="0"/>
		<edge from-layer="240" from-port="0" to-layer="241" to-port="1"/>
		<edge from-layer="241" from-port="2" to-layer="243" to-port="0"/>
		<edge from-layer="242" from-port="0" to-layer="243" to-port="1"/>
		<edge from-layer="243" from-port="2" to-layer="245" to-port="0"/>
		<edge from-layer="244" from-port="0" to-layer="245" to-port="1"/>
		<edge from-layer="245" from-port="2" to-layer="246" to-port="0"/>
		<edge from-layer="246" from-port="1" to-layer="248" to-port="0"/>
		<edge from-layer="247" from-port="0" to-layer="248" to-port="1"/>
		<edge from-layer="243" from-port="2" to-layer="249" to-port="0"/>
		<edge from-layer="248" from-port="2" to-layer="249" to-port="1"/>
		<edge from-layer="249" from-port="2" to-layer="251" to-port="0"/>
		<edge from-layer="250" from-port="0" to-layer="251" to-port="1"/>
		<edge from-layer="251" from-port="2" to-layer="253" to-port="0"/>
		<edge from-layer="252" from-port="0" to-layer="253" to-port="1"/>
		<edge from-layer="239" from-port="2" to-layer="254" to-port="0"/>
		<edge from-layer="254" from-port="1" to-layer="256" to-port="0"/>
		<edge from-layer="255" from-port="0" to-layer="256" to-port="1"/>
		<edge from-layer="256" from-port="2" to-layer="258" to-port="0"/>
		<edge from-layer="257" from-port="0" to-layer="258" to-port="1"/>
		<edge from-layer="258" from-port="2" to-layer="260" to-port="0"/>
		<edge from-layer="259" from-port="0" to-layer="260" to-port="1"/>
		<edge from-layer="260" from-port="2" to-layer="261" to-port="0"/>
		<edge from-layer="261" from-port="1" to-layer="263" to-port="0"/>
		<edge from-layer="262" from-port="0" to-layer="263" to-port="1"/>
		<edge from-layer="258" from-port="2" to-layer="264" to-port="0"/>
		<edge from-layer="263" from-port="2" to-layer="264" to-port="1"/>
		<edge from-layer="264" from-port="2" to-layer="266" to-port="0"/>
		<edge from-layer="265" from-port="0" to-layer="266" to-port="1"/>
		<edge from-layer="266" from-port="2" to-layer="268" to-port="0"/>
		<edge from-layer="267" from-port="0" to-layer="268" to-port="1"/>
		<edge from-layer="253" from-port="2" to-layer="269" to-port="0"/>
		<edge from-layer="268" from-port="2" to-layer="269" to-port="1"/>
		<edge from-layer="269" from-port="2" to-layer="270" to-port="0"/>
		<edge from-layer="270" from-port="1" to-layer="272" to-port="0"/>
		<edge from-layer="271" from-port="0" to-layer="272" to-port="1"/>
		<edge from-layer="272" from-port="2" to-layer="273" to-port="0"/>
		<edge from-layer="239" from-port="2" to-layer="273" to-port="1"/>
		<edge from-layer="273" from-port="2" to-layer="275" to-port="0"/>
		<edge from-layer="274" from-port="0" to-layer="275" to-port="1"/>
		<edge from-layer="275" from-port="2" to-layer="277" to-port="0"/>
		<edge from-layer="276" from-port="0" to-layer="277" to-port="1"/>
		<edge from-layer="277" from-port="2" to-layer="279" to-port="0"/>
		<edge from-layer="278" from-port="0" to-layer="279" to-port="1"/>
		<edge from-layer="279" from-port="2" to-layer="280" to-port="0"/>
		<edge from-layer="280" from-port="1" to-layer="282" to-port="0"/>
		<edge from-layer="281" from-port="0" to-layer="282" to-port="1"/>
		<edge from-layer="277" from-port="2" to-layer="283" to-port="0"/>
		<edge from-layer="282" from-port="2" to-layer="283" to-port="1"/>
		<edge from-layer="283" from-port="2" to-layer="285" to-port="0"/>
		<edge from-layer="284" from-port="0" to-layer="285" to-port="1"/>
		<edge from-layer="285" from-port="2" to-layer="287" to-port="0"/>
		<edge from-layer="286" from-port="0" to-layer="287" to-port="1"/>
		<edge from-layer="287" from-port="2" to-layer="288" to-port="0"/>
		<edge from-layer="288" from-port="1" to-layer="290" to-port="0"/>
		<edge from-layer="289" from-port="0" to-layer="290" to-port="1"/>
		<edge from-layer="290" from-port="2" to-layer="292" to-port="0"/>
		<edge from-layer="291" from-port="0" to-layer="292" to-port="1"/>
		<edge from-layer="292" from-port="2" to-layer="293" to-port="0"/>
		<edge from-layer="293" from-port="1" to-layer="295" to-port="0"/>
		<edge from-layer="294" from-port="0" to-layer="295" to-port="1"/>
		<edge from-layer="295" from-port="2" to-layer="297" to-port="0"/>
		<edge from-layer="296" from-port="0" to-layer="297" to-port="1"/>
		<edge from-layer="297" from-port="2" to-layer="298" to-port="0"/>
		<edge from-layer="298" from-port="1" to-layer="300" to-port="0"/>
		<edge from-layer="299" from-port="0" to-layer="300" to-port="1"/>
		<edge from-layer="300" from-port="2" to-layer="301" to-port="0"/>
		<edge from-layer="287" from-port="2" to-layer="301" to-port="1"/>
		<edge from-layer="301" from-port="2" to-layer="303" to-port="0"/>
		<edge from-layer="302" from-port="0" to-layer="303" to-port="1"/>
		<edge from-layer="303" from-port="2" to-layer="304" to-port="0"/>
		<edge from-layer="304" from-port="1" to-layer="306" to-port="0"/>
		<edge from-layer="305" from-port="0" to-layer="306" to-port="1"/>
		<edge from-layer="301" from-port="2" to-layer="307" to-port="0"/>
		<edge from-layer="306" from-port="2" to-layer="307" to-port="1"/>
		<edge from-layer="307" from-port="2" to-layer="309" to-port="0"/>
		<edge from-layer="308" from-port="0" to-layer="309" to-port="1"/>
		<edge from-layer="309" from-port="2" to-layer="311" to-port="0"/>
		<edge from-layer="310" from-port="0" to-layer="311" to-port="1"/>
		<edge from-layer="311" from-port="2" to-layer="312" to-port="0"/>
		<edge from-layer="312" from-port="1" to-layer="314" to-port="0"/>
		<edge from-layer="313" from-port="0" to-layer="314" to-port="1"/>
		<edge from-layer="314" from-port="2" to-layer="316" to-port="0"/>
		<edge from-layer="315" from-port="0" to-layer="316" to-port="1"/>
		<edge from-layer="316" from-port="2" to-layer="318" to-port="0"/>
		<edge from-layer="317" from-port="0" to-layer="318" to-port="1"/>
		<edge from-layer="318" from-port="2" to-layer="319" to-port="0"/>
		<edge from-layer="319" from-port="1" to-layer="321" to-port="0"/>
		<edge from-layer="320" from-port="0" to-layer="321" to-port="1"/>
		<edge from-layer="316" from-port="2" to-layer="322" to-port="0"/>
		<edge from-layer="321" from-port="2" to-layer="322" to-port="1"/>
		<edge from-layer="322" from-port="2" to-layer="324" to-port="0"/>
		<edge from-layer="323" from-port="0" to-layer="324" to-port="1"/>
		<edge from-layer="324" from-port="2" to-layer="326" to-port="0"/>
		<edge from-layer="325" from-port="0" to-layer="326" to-port="1"/>
		<edge from-layer="326" from-port="2" to-layer="327" to-port="0"/>
		<edge from-layer="327" from-port="1" to-layer="329" to-port="0"/>
		<edge from-layer="328" from-port="0" to-layer="329" to-port="1"/>
		<edge from-layer="329" from-port="2" to-layer="331" to-port="0"/>
		<edge from-layer="330" from-port="0" to-layer="331" to-port="1"/>
		<edge from-layer="331" from-port="2" to-layer="332" to-port="0"/>
		<edge from-layer="332" from-port="1" to-layer="334" to-port="0"/>
		<edge from-layer="333" from-port="0" to-layer="334" to-port="1"/>
		<edge from-layer="334" from-port="2" to-layer="336" to-port="0"/>
		<edge from-layer="335" from-port="0" to-layer="336" to-port="1"/>
		<edge from-layer="336" from-port="2" to-layer="337" to-port="0"/>
		<edge from-layer="337" from-port="1" to-layer="339" to-port="0"/>
		<edge from-layer="338" from-port="0" to-layer="339" to-port="1"/>
		<edge from-layer="339" from-port="2" to-layer="340" to-port="0"/>
		<edge from-layer="326" from-port="2" to-layer="340" to-port="1"/>
		<edge from-layer="340" from-port="2" to-layer="342" to-port="0"/>
		<edge from-layer="341" from-port="0" to-layer="342" to-port="1"/>
		<edge from-layer="342" from-port="2" to-layer="343" to-port="0"/>
		<edge from-layer="343" from-port="1" to-layer="345" to-port="0"/>
		<edge from-layer="344" from-port="0" to-layer="345" to-port="1"/>
		<edge from-layer="340" from-port="2" to-layer="346" to-port="0"/>
		<edge from-layer="345" from-port="2" to-layer="346" to-port="1"/>
		<edge from-layer="346" from-port="2" to-layer="348" to-port="0"/>
		<edge from-layer="347" from-port="0" to-layer="348" to-port="1"/>
		<edge from-layer="348" from-port="2" to-layer="350" to-port="0"/>
		<edge from-layer="349" from-port="0" to-layer="350" to-port="1"/>
		<edge from-layer="312" from-port="1" to-layer="351" to-port="0"/>
		<edge from-layer="350" from-port="2" to-layer="351" to-port="1"/>
		<edge from-layer="351" from-port="2" to-layer="353" to-port="0"/>
		<edge from-layer="352" from-port="0" to-layer="353" to-port="1"/>
		<edge from-layer="353" from-port="2" to-layer="355" to-port="0"/>
		<edge from-layer="354" from-port="0" to-layer="355" to-port="1"/>
		<edge from-layer="355" from-port="2" to-layer="357" to-port="0"/>
		<edge from-layer="356" from-port="0" to-layer="357" to-port="1"/>
		<edge from-layer="357" from-port="2" to-layer="358" to-port="0"/>
		<edge from-layer="358" from-port="1" to-layer="360" to-port="0"/>
		<edge from-layer="359" from-port="0" to-layer="360" to-port="1"/>
		<edge from-layer="355" from-port="2" to-layer="361" to-port="0"/>
		<edge from-layer="360" from-port="2" to-layer="361" to-port="1"/>
		<edge from-layer="361" from-port="2" to-layer="363" to-port="0"/>
		<edge from-layer="362" from-port="0" to-layer="363" to-port="1"/>
		<edge from-layer="363" from-port="2" to-layer="365" to-port="0"/>
		<edge from-layer="364" from-port="0" to-layer="365" to-port="1"/>
		<edge from-layer="365" from-port="2" to-layer="366" to-port="0"/>
		<edge from-layer="366" from-port="1" to-layer="368" to-port="0"/>
		<edge from-layer="367" from-port="0" to-layer="368" to-port="1"/>
		<edge from-layer="368" from-port="2" to-layer="370" to-port="0"/>
		<edge from-layer="369" from-port="0" to-layer="370" to-port="1"/>
		<edge from-layer="370" from-port="2" to-layer="371" to-port="0"/>
		<edge from-layer="371" from-port="1" to-layer="373" to-port="0"/>
		<edge from-layer="372" from-port="0" to-layer="373" to-port="1"/>
		<edge from-layer="373" from-port="2" to-layer="375" to-port="0"/>
		<edge from-layer="374" from-port="0" to-layer="375" to-port="1"/>
		<edge from-layer="375" from-port="2" to-layer="376" to-port="0"/>
		<edge from-layer="376" from-port="1" to-layer="378" to-port="0"/>
		<edge from-layer="377" from-port="0" to-layer="378" to-port="1"/>
		<edge from-layer="378" from-port="2" to-layer="379" to-port="0"/>
		<edge from-layer="365" from-port="2" to-layer="379" to-port="1"/>
		<edge from-layer="379" from-port="2" to-layer="381" to-port="0"/>
		<edge from-layer="380" from-port="0" to-layer="381" to-port="1"/>
		<edge from-layer="381" from-port="2" to-layer="382" to-port="0"/>
		<edge from-layer="382" from-port="1" to-layer="384" to-port="0"/>
		<edge from-layer="383" from-port="0" to-layer="384" to-port="1"/>
		<edge from-layer="379" from-port="2" to-layer="385" to-port="0"/>
		<edge from-layer="384" from-port="2" to-layer="385" to-port="1"/>
		<edge from-layer="385" from-port="2" to-layer="387" to-port="0"/>
		<edge from-layer="386" from-port="0" to-layer="387" to-port="1"/>
		<edge from-layer="387" from-port="2" to-layer="389" to-port="0"/>
		<edge from-layer="388" from-port="0" to-layer="389" to-port="1"/>
		<edge from-layer="389" from-port="2" to-layer="391" to-port="0"/>
		<edge from-layer="390" from-port="0" to-layer="391" to-port="1"/>
		<edge from-layer="391" from-port="2" to-layer="393" to-port="0"/>
		<edge from-layer="392" from-port="0" to-layer="393" to-port="1"/>
		<edge from-layer="393" from-port="2" to-layer="395" to-port="0"/>
		<edge from-layer="394" from-port="0" to-layer="395" to-port="1"/>
		<edge from-layer="395" from-port="2" to-layer="396" to-port="0"/>
		<edge from-layer="396" from-port="1" to-layer="398" to-port="0"/>
		<edge from-layer="397" from-port="0" to-layer="398" to-port="1"/>
		<edge from-layer="393" from-port="2" to-layer="399" to-port="0"/>
		<edge from-layer="398" from-port="2" to-layer="399" to-port="1"/>
		<edge from-layer="399" from-port="2" to-layer="401" to-port="0"/>
		<edge from-layer="400" from-port="0" to-layer="401" to-port="1"/>
		<edge from-layer="401" from-port="2" to-layer="403" to-port="0"/>
		<edge from-layer="402" from-port="0" to-layer="403" to-port="1"/>
		<edge from-layer="389" from-port="2" to-layer="404" to-port="0"/>
		<edge from-layer="404" from-port="1" to-layer="406" to-port="0"/>
		<edge from-layer="405" from-port="0" to-layer="406" to-port="1"/>
		<edge from-layer="406" from-port="2" to-layer="408" to-port="0"/>
		<edge from-layer="407" from-port="0" to-layer="408" to-port="1"/>
		<edge from-layer="408" from-port="2" to-layer="410" to-port="0"/>
		<edge from-layer="409" from-port="0" to-layer="410" to-port="1"/>
		<edge from-layer="410" from-port="2" to-layer="411" to-port="0"/>
		<edge from-layer="411" from-port="1" to-layer="413" to-port="0"/>
		<edge from-layer="412" from-port="0" to-layer="413" to-port="1"/>
		<edge from-layer="408" from-port="2" to-layer="414" to-port="0"/>
		<edge from-layer="413" from-port="2" to-layer="414" to-port="1"/>
		<edge from-layer="414" from-port="2" to-layer="416" to-port="0"/>
		<edge from-layer="415" from-port="0" to-layer="416" to-port="1"/>
		<edge from-layer="416" from-port="2" to-layer="418" to-port="0"/>
		<edge from-layer="417" from-port="0" to-layer="418" to-port="1"/>
		<edge from-layer="403" from-port="2" to-layer="419" to-port="0"/>
		<edge from-layer="418" from-port="2" to-layer="419" to-port="1"/>
		<edge from-layer="419" from-port="2" to-layer="420" to-port="0"/>
		<edge from-layer="420" from-port="1" to-layer="422" to-port="0"/>
		<edge from-layer="421" from-port="0" to-layer="422" to-port="1"/>
		<edge from-layer="422" from-port="2" to-layer="423" to-port="0"/>
		<edge from-layer="389" from-port="2" to-layer="423" to-port="1"/>
		<edge from-layer="423" from-port="2" to-layer="425" to-port="0"/>
		<edge from-layer="424" from-port="0" to-layer="425" to-port="1"/>
		<edge from-layer="425" from-port="2" to-layer="427" to-port="0"/>
		<edge from-layer="426" from-port="0" to-layer="427" to-port="1"/>
		<edge from-layer="427" from-port="2" to-layer="429" to-port="0"/>
		<edge from-layer="428" from-port="0" to-layer="429" to-port="1"/>
		<edge from-layer="429" from-port="2" to-layer="430" to-port="0"/>
		<edge from-layer="430" from-port="1" to-layer="432" to-port="0"/>
		<edge from-layer="431" from-port="0" to-layer="432" to-port="1"/>
		<edge from-layer="427" from-port="2" to-layer="433" to-port="0"/>
		<edge from-layer="432" from-port="2" to-layer="433" to-port="1"/>
		<edge from-layer="433" from-port="2" to-layer="435" to-port="0"/>
		<edge from-layer="434" from-port="0" to-layer="435" to-port="1"/>
		<edge from-layer="435" from-port="2" to-layer="437" to-port="0"/>
		<edge from-layer="436" from-port="0" to-layer="437" to-port="1"/>
		<edge from-layer="437" from-port="2" to-layer="438" to-port="0"/>
		<edge from-layer="438" from-port="1" to-layer="440" to-port="0"/>
		<edge from-layer="439" from-port="0" to-layer="440" to-port="1"/>
		<edge from-layer="440" from-port="2" to-layer="442" to-port="0"/>
		<edge from-layer="441" from-port="0" to-layer="442" to-port="1"/>
		<edge from-layer="442" from-port="2" to-layer="443" to-port="0"/>
		<edge from-layer="443" from-port="1" to-layer="445" to-port="0"/>
		<edge from-layer="444" from-port="0" to-layer="445" to-port="1"/>
		<edge from-layer="445" from-port="2" to-layer="447" to-port="0"/>
		<edge from-layer="446" from-port="0" to-layer="447" to-port="1"/>
		<edge from-layer="447" from-port="2" to-layer="448" to-port="0"/>
		<edge from-layer="448" from-port="1" to-layer="450" to-port="0"/>
		<edge from-layer="449" from-port="0" to-layer="450" to-port="1"/>
		<edge from-layer="450" from-port="2" to-layer="451" to-port="0"/>
		<edge from-layer="437" from-port="2" to-layer="451" to-port="1"/>
		<edge from-layer="451" from-port="2" to-layer="453" to-port="0"/>
		<edge from-layer="452" from-port="0" to-layer="453" to-port="1"/>
		<edge from-layer="453" from-port="2" to-layer="454" to-port="0"/>
		<edge from-layer="454" from-port="1" to-layer="456" to-port="0"/>
		<edge from-layer="455" from-port="0" to-layer="456" to-port="1"/>
		<edge from-layer="451" from-port="2" to-layer="457" to-port="0"/>
		<edge from-layer="456" from-port="2" to-layer="457" to-port="1"/>
		<edge from-layer="457" from-port="2" to-layer="459" to-port="0"/>
		<edge from-layer="458" from-port="0" to-layer="459" to-port="1"/>
		<edge from-layer="459" from-port="2" to-layer="461" to-port="0"/>
		<edge from-layer="460" from-port="0" to-layer="461" to-port="1"/>
		<edge from-layer="461" from-port="2" to-layer="463" to-port="0"/>
		<edge from-layer="462" from-port="0" to-layer="463" to-port="1"/>
		<edge from-layer="463" from-port="2" to-layer="465" to-port="0"/>
		<edge from-layer="464" from-port="0" to-layer="465" to-port="1"/>
		<edge from-layer="465" from-port="2" to-layer="467" to-port="0"/>
		<edge from-layer="466" from-port="0" to-layer="467" to-port="1"/>
		<edge from-layer="467" from-port="2" to-layer="468" to-port="0"/>
		<edge from-layer="468" from-port="1" to-layer="470" to-port="0"/>
		<edge from-layer="469" from-port="0" to-layer="470" to-port="1"/>
		<edge from-layer="465" from-port="2" to-layer="471" to-port="0"/>
		<edge from-layer="470" from-port="2" to-layer="471" to-port="1"/>
		<edge from-layer="471" from-port="2" to-layer="473" to-port="0"/>
		<edge from-layer="472" from-port="0" to-layer="473" to-port="1"/>
		<edge from-layer="473" from-port="2" to-layer="475" to-port="0"/>
		<edge from-layer="474" from-port="0" to-layer="475" to-port="1"/>
		<edge from-layer="475" from-port="2" to-layer="476" to-port="0"/>
		<edge from-layer="476" from-port="1" to-layer="478" to-port="0"/>
		<edge from-layer="477" from-port="0" to-layer="478" to-port="1"/>
		<edge from-layer="478" from-port="2" to-layer="480" to-port="0"/>
		<edge from-layer="479" from-port="0" to-layer="480" to-port="1"/>
		<edge from-layer="480" from-port="2" to-layer="481" to-port="0"/>
		<edge from-layer="481" from-port="1" to-layer="483" to-port="0"/>
		<edge from-layer="482" from-port="0" to-layer="483" to-port="1"/>
		<edge from-layer="483" from-port="2" to-layer="485" to-port="0"/>
		<edge from-layer="484" from-port="0" to-layer="485" to-port="1"/>
		<edge from-layer="485" from-port="2" to-layer="486" to-port="0"/>
		<edge from-layer="486" from-port="1" to-layer="488" to-port="0"/>
		<edge from-layer="487" from-port="0" to-layer="488" to-port="1"/>
		<edge from-layer="488" from-port="2" to-layer="489" to-port="0"/>
		<edge from-layer="475" from-port="2" to-layer="489" to-port="1"/>
		<edge from-layer="489" from-port="2" to-layer="491" to-port="0"/>
		<edge from-layer="490" from-port="0" to-layer="491" to-port="1"/>
		<edge from-layer="491" from-port="2" to-layer="492" to-port="0"/>
		<edge from-layer="492" from-port="1" to-layer="494" to-port="0"/>
		<edge from-layer="493" from-port="0" to-layer="494" to-port="1"/>
		<edge from-layer="489" from-port="2" to-layer="495" to-port="0"/>
		<edge from-layer="494" from-port="2" to-layer="495" to-port="1"/>
		<edge from-layer="495" from-port="2" to-layer="497" to-port="0"/>
		<edge from-layer="496" from-port="0" to-layer="497" to-port="1"/>
		<edge from-layer="497" from-port="2" to-layer="499" to-port="0"/>
		<edge from-layer="498" from-port="0" to-layer="499" to-port="1"/>
		<edge from-layer="461" from-port="2" to-layer="500" to-port="0"/>
		<edge from-layer="499" from-port="2" to-layer="500" to-port="1"/>
		<edge from-layer="500" from-port="2" to-layer="502" to-port="0"/>
		<edge from-layer="501" from-port="0" to-layer="502" to-port="1"/>
		<edge from-layer="502" from-port="2" to-layer="504" to-port="0"/>
		<edge from-layer="503" from-port="0" to-layer="504" to-port="1"/>
		<edge from-layer="504" from-port="2" to-layer="506" to-port="0"/>
		<edge from-layer="505" from-port="0" to-layer="506" to-port="1"/>
		<edge from-layer="506" from-port="2" to-layer="507" to-port="0"/>
		<edge from-layer="507" from-port="1" to-layer="509" to-port="0"/>
		<edge from-layer="508" from-port="0" to-layer="509" to-port="1"/>
		<edge from-layer="504" from-port="2" to-layer="510" to-port="0"/>
		<edge from-layer="509" from-port="2" to-layer="510" to-port="1"/>
		<edge from-layer="510" from-port="2" to-layer="511" to-port="0"/>
		<edge from-layer="511" from-port="1" to-layer="513" to-port="0"/>
		<edge from-layer="512" from-port="0" to-layer="513" to-port="1"/>
		<edge from-layer="513" from-port="2" to-layer="515" to-port="0"/>
		<edge from-layer="514" from-port="0" to-layer="515" to-port="1"/>
		<edge from-layer="515" from-port="2" to-layer="517" to-port="0"/>
		<edge from-layer="516" from-port="0" to-layer="517" to-port="1"/>
		<edge from-layer="517" from-port="2" to-layer="519" to-port="0"/>
		<edge from-layer="518" from-port="0" to-layer="519" to-port="1"/>
		<edge from-layer="521" from-port="0" to-layer="522" to-port="0"/>
		<edge from-layer="522" from-port="1" to-layer="525" to-port="0"/>
		<edge from-layer="523" from-port="0" to-layer="525" to-port="1"/>
		<edge from-layer="524" from-port="0" to-layer="525" to-port="2"/>
		<edge from-layer="520" from-port="0" to-layer="526" to-port="0"/>
		<edge from-layer="525" from-port="3" to-layer="526" to-port="1"/>
		<edge from-layer="519" from-port="2" to-layer="527" to-port="0"/>
		<edge from-layer="526" from-port="2" to-layer="527" to-port="1"/>
		<edge from-layer="527" from-port="2" to-layer="528" to-port="0"/>
		<edge from-layer="521" from-port="0" to-layer="528" to-port="1"/>
		<edge from-layer="528" from-port="2" to-layer="530" to-port="0"/>
		<edge from-layer="529" from-port="0" to-layer="530" to-port="1"/>
		<edge from-layer="530" from-port="2" to-layer="531" to-port="0"/>
	</edges>
	<meta_data>
		<MO_version value="2021.3.0-2767-91fe355156e-releases/2021/3"/>
		<cli_parameters>
			<caffe_parser_path value="DIR"/>
			<data_type value="FP32"/>
			<disable_nhwc_to_nchw value="False"/>
			<disable_omitting_optional value="False"/>
			<disable_resnet_optimization value="False"/>
			<disable_weights_compression value="False"/>
			<enable_concat_optimization value="False"/>
			<enable_flattening_nested_params value="False"/>
			<enable_ssd_gluoncv value="False"/>
			<extensions value="DIR"/>
			<framework value="onnx"/>
			<freeze_placeholder_with_value value="{}"/>
			<generate_deprecated_IR_V7 value="False"/>
			<input value="input"/>
			<input_model value="DIR/v199_e34.onnx"/>
			<input_model_is_text value="False"/>
			<input_shape value="[1,3,16,224,224]"/>
			<k value="DIR/CustomLayersMapping.xml"/>
			<keep_shape_ops value="True"/>
			<legacy_mxnet_model value="False"/>
			<log_level value="ERROR"/>
			<mean_scale_values value="{'input': {'mean': array([123.675, 116.28 , 103.53 ]), 'scale': array([58.39, 57.12, 57.38])}}"/>
			<mean_values value="input[123.675,116.28,103.53]"/>
			<model_name value="asl-recognition-0004"/>
			<output value="['output']"/>
			<output_dir value="DIR"/>
			<placeholder_data_types value="{}"/>
			<placeholder_shapes value="{'input': array([  1,   3,  16, 224, 224])}"/>
			<progress value="False"/>
			<remove_memory value="False"/>
			<remove_output_softmax value="False"/>
			<reverse_input_channels value="False"/>
			<save_params_from_nd value="False"/>
			<scale_values value="input[58.39,57.12,57.38]"/>
			<silent value="False"/>
			<static_shape value="False"/>
			<stream_output value="False"/>
			<unset unset_cli_parameters="batch, counts, disable_fusing, disable_gfusing, finegrain_fusing, input_checkpoint, input_meta_graph, input_proto, input_symbol, mean_file, mean_file_offsets, move_to_preprocess, nd_prefix_name, pretrained_model_name, saved_model_dir, saved_model_tags, scale, tensorboard_logdir, tensorflow_custom_layer_libraries, tensorflow_custom_operations_config_update, tensorflow_object_detection_api_pipeline_config, tensorflow_use_custom_operations_config, transformations_config"/>
		</cli_parameters>
	</meta_data>
</net>
